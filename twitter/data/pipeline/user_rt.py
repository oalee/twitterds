from ..loader.spark import get_retweets_session
import os
from pyspark.sql.functions import col, explode

from yerbamate import env

# env = yerbamate.Environment()


period = env.get_hparam("period", "after")
print(period)
# import ipdb; ipdb.set_trace()

if period == "after":
    time_filter = (col("date") > "2022-09-14") & (col("date") < "2023-04-17")

elif period == "before":
    time_filter = (col("date") > "2022-01-01") & (col("date") < "2022-09-14")

else:
    raise ValueError("Invalid period")

columns = ["userId", "retweetedTweetUserId", "date"]

parquet_df = get_retweets_session(columns)

# this is spark dataframe
# drop null hashtags
parquet_df.na.drop(subset=["retweetedTweetUserId"])

# date after 2022
parquet_df = parquet_df.filter(time_filter)

# drop same pairs of userId and retweetedUserId
parquet_df = parquet_df.dropDuplicates(["userId", "retweetedTweetUserId"])


# drop retweetedTweetUserIds that are not in userIds
# drop retweetedTweetUserIds that are not in userIds
# Create a dataframe of distinct userIds
distinct_user_ids_df = parquet_df.select("userId").distinct()

# Rename the column in the new dataframe for the join operation
distinct_user_ids_df = distinct_user_ids_df.withColumnRenamed("userId", "userIdDistinct")

# Join the dataframes on the condition that retweetedTweetUserId matches userIdDistinct
parquet_df = parquet_df.join(distinct_user_ids_df, parquet_df.retweetedTweetUserId == distinct_user_ids_df.userIdDistinct)

# Now the DataFrame 'parquet_df' only includes rows where 'retweetedTweetUserId' exists in 'userIds'.
# Drop the extra column generated by the join operation
parquet_df = parquet_df.drop("userIdDistinct")

# save parquet, mode overwrite if exists
parquet_df.write.mode("overwrite").parquet(
    os.path.join(env["save"], f"user_rt_{period}.parquet")
)
